# # -*- coding: utf-8 -*-
# """Create NLP App Streamlit.ipynb

# Automatically generated by Colab.

# # ğŸ“š Lab 3: Create NLP App with 3 Hugging Face Models (Streamlit Version)
# """

# # âœ¨ Install Required Libraries
# # !pip install streamlit transformers torch

# # âœ¨ Import Required Libraries

# import streamlit as st
# from transformers import pipeline

# # âœ¨ Load Hugging Face Pipelines
# # 1. Text Generation Pipeline (gpt2)
# text_generator = pipeline("text-generation", model="openai-community/gpt2") 

# # 2. Summarization Pipeline (default model)
# summarizer = pipeline("summarization", model="google-t5/t5-small")

# # 3. Translation Pipeline (English â” French)
# translator = pipeline("translation_en_to_fr", model="Helsinki-NLP/opus-mt-en-fr")

# question_answering = pipeline("question-answering", model="deepset/roberta-base-squad2")


# # âœ¨ Create the Streamlit App

# # Title
# st.title(" NLP App using 4 Hugging Face Models ")

# # Instruction
# st.write("Start with a short topic and let's work in three steps: Generate text about it â” Summarize â” Translate â”  question_answering")

# # User input
# user_input = st.text_area(" Write a topic: ")
# if "generated_text" not in st.session_state:
#     st.session_state['generated_text'] = "" 
# # Button to process
# if st.button("Start processing..."):
#     if user_input.strip() != "":
#         with st.spinner("Wait ...!â³"):

#             # Step 1: Generate Text
#             generated_output = text_generator(user_input, max_length=100, num_return_sequences=1)
#             generated_text = generated_output[0]['generated_text']
#             st.subheader("ğŸ”¹generated_text: ")
#             st.write(generated_text)
#             st.session_state['generated_text'] = generated_text 

#             # Step 2: Summarize the Generated Text
#             summarized_output = summarizer(generated_text, max_length=50, min_length=25, do_sample=False)
#             summarized_text = summarized_output[0]['summary_text']
#             st.subheader("ğŸ”¹ summary_text: ")
#             st.write(summarized_text)

#             # Step 3: Translate the Summarized Text
#             translated_output = translator(summarized_text)
#             translated_text = translated_output[0]['translation_text']
#             st.subheader("ğŸ”¹ translation_text to fr: ")
#             st.write(translated_text)
   

#     else:
#         st.warning("âš ï¸ try text first")

# if st.session_state['generated_text']:
#     question = st.text_area("Write a question:")

#     if st.button("Get Answer"):
#         if question.strip() != "":
#                 answer = question_answering(question= question,context = st.session_state['generated_text'] )
#                 answering = answer['answer']
#                 st.subheader("ğŸ”¹ Question Answering:")

#                 st.write(answering)
#         else:
#             st.warning("âš ï¸ Please enter a question.")

# 



import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# âœ¨ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆØ§Ù„ØªÙˆÙƒÙ†ÙŠØ²Ø± ÙŠØ¯ÙˆÙŠÙ‹Ø§ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù‡Ø§Ø²
model_name = "openai-community/gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù‡Ø§Ø²: 0 Ù„Ùˆ ÙÙŠ CUDAØŒ -1 ÙŠØ¹Ù†ÙŠ CPU
device = 0 if torch.cuda.is_available() else -1

# Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„Ù„Ù†ØµÙˆØµ
text_generator = pipeline("text-generation", model=model, tokenizer=tokenizer, device=device)

# Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø²ÙŠ Ù…Ø§ Ù‡ÙŠ
from transformers import pipeline
summarizer = pipeline("summarization", model="google-t5/t5-small")
translator = pipeline("translation_en_to_fr", model="Helsinki-NLP/opus-mt-en-fr")
question_answering = pipeline("question-answering", model="deepset/roberta-base-squad2")

# âœ¨ ØªØ·Ø¨ÙŠÙ‚ Streamlit
st.title(" NLP App using 4 Hugging Face Models ")
st.write("Start with a short topic and let's work in three steps: Generate text about it â” Summarize â” Translate â”  question_answering")

user_input = st.text_area(" Write a topic: ")
if "generated_text" not in st.session_state:
    st.session_state['generated_text'] = ""

if st.button("Start processing..."):
    if user_input.strip() != "":
        with st.spinner("Wait ...!â³"):
            generated_output = text_generator(user_input, max_length=100, num_return_sequences=1)
            generated_text = generated_output[0]['generated_text']
            st.subheader("ğŸ”¹generated_text: ")
            st.write(generated_text)
            st.session_state['generated_text'] = generated_text

            summarized_output = summarizer(generated_text, max_length=50, min_length=25, do_sample=False)
            summarized_text = summarized_output[0]['summary_text']
            st.subheader("ğŸ”¹ summary_text: ")
            st.write(summarized_text)

            translated_output = translator(summarized_text)
            translated_text = translated_output[0]['translation_text']
            st.subheader("ğŸ”¹ translation_text to fr: ")
            st.write(translated_text)
    else:
        st.warning("âš ï¸ try text first")

if st.session_state['generated_text']:
    question = st.text_area("Write a question:")
    if st.button("Get Answer"):
        if question.strip() != "":
            answer = question_answering(question=question, context=st.session_state['generated_text'])
            st.subheader("ğŸ”¹ Question Answering:")
            st.write(answer['answer'])
        else:
            st.warning("âš ï¸ Please enter a question.")
